# Technical Implementation Plan: Multimodal AI Video Editor Agent

## 1. Project Summary

**Goal**: Build "Cursor for Video Editors"—an AI-native video editing assistant that integrates directly into a professional non-linear editing (NLE) workflow. The system enables users to edit video through high-level natural language intent (e.g., "Make this sequence feel more cinematic") which the agent translates into precise, frame-perfect editing operations.

**Core Value Proposition**:
- **Semantic Editing**: Move beyond manual timeline manipulation to intent-based editing.
- **Context Awareness**: The agent "sees" and "hears" the footage, allowing for content-aware decisions (e.g., "cut on the beat," "remove the blurry shot").
- **Explainable AI**: Every automated cut or effect comes with a "reasoning trace," allowing users to query, learn from, and refine the AI's creative choices.

---

## 2. Technical Architecture & Stack

### Stack Selection

| Component | Technology | Rationale |
|-----------|------------|-----------|
| **Core Editor Engine** | **Shotcut (MLT Framework) + Python Bindings** | Shotcut is a proven, open-source C++/Qt editor built on the MLT framework. MLT provides robust Python bindings, allowing us to programmatically manipulate the timeline, apply filters, and render previews without reinventing the rendering engine. |
| **Orchestration** | **LangGraph** | We need a cyclic, stateful control flow to handle the "Plan → Validate → Execute → Feedback" loop. LangGraph allows us to model the editing process as a graph where the agent can loop back to refine its edit list based on validation errors or user feedback. |
| **Vision Model** | **CLIP (OpenAI) / SigLIP (Google)** | For fast semantic search and scene understanding (e.g., matching "cinematic" to visual embeddings). These models are efficient enough to run locally or via API for frame indexing. |
| **Audio Model** | **Whisper (OpenAI) + CLAP** | Whisper provides precise timestamped transcription for dialogue editing. CLAP (Contrastive Language-Audio Pretraining) allows us to classify audio events (music, ambient noise) to align cuts with audio cues. |
| **Vector Database** | **ChromaDB** | A local-first vector store to index video frames and audio segments. This enables the agent to query "Where are the happy moments?" by searching embeddings. |
| **Communication** | **ZeroMQ (ZMQ)** | High-performance asynchronous messaging to bridge the C++ (Qt) UI of Shotcut and the Python AI sidecar process. It decouples the heavy AI inference from the UI rendering thread. |

### Architecture Diagram

```mermaid
graph TD
    subgraph "Editor Layer (Forked Shotcut)"
        UI[Qt User Interface] -->|User Prompt| Bridge
        UI -->|Timeline State| Bridge
        Renderer[MLT Renderer]
    end

    subgraph "Bridge (ZeroMQ)"
        Bridge[IPC Messaging Layer]
    end

    subgraph "AI Agent Layer (Python)"
        Bridge -->|Request| Controller[LangGraph Controller]
        
        subgraph "Perception Engine"
            Ingest[Video Ingester] -->|Frames| V_Model[Vision Model (CLIP)]
            Ingest -->|Audio| A_Model[Audio Model (Whisper/CLAP)]
            V_Model --> VectorDB[(ChromaDB)]
            A_Model --> VectorDB
        end
        
        subgraph "Reasoning Engine"
            Controller --> Planner[Edit Planner LLM]
            Planner -->|Draft EDL| Validator[Narrative Validator]
            Validator -->|Approved EDL| Executor[Command Translator]
        end
        
        Controller -->|Query Context| VectorDB
        Executor -->|MLT Commands| Bridge
    end

    Bridge -->|Update Timeline| Renderer
```

### Data Schemas

**1. User Intent**
```python
from pydantic import BaseModel
from typing import List, Optional, Literal

class UserIntent(BaseModel):
    raw_prompt: str
    intent_type: Literal["style_transfer", "pacing", "content_cut", "correction"]
    target_segments: Optional[List[tuple[float, float]]] = None  # Start/End timestamps
    parameters: dict  # e.g., {"speed": 0.8, "lut": "teal_orange"}
```

**2. Timeline State (The "World Model")**
```python
class TimelineItem(BaseModel):
    id: str
    source_file: str
    in_point: float
    out_point: float
    track_index: int
    filters: List[dict]
    embedding_id: str  # Link to vector store

class EditDecisionList(BaseModel):
    """The plan generated by the AI before execution"""
    project_id: str
    operations: List[dict]  # e.g., {"action": "split", "time": 12.5}
    reasoning_trace: str    # "Cut at 12.5s because dialogue ended and beat dropped"
```

**3. Multimodal Index**
```python
class SceneNode(BaseModel):
    """Stored in ChromaDB"""
    timestamp: float
    visual_embedding: List[float]  # CLIP vector
    audio_embedding: List[float]   # CLAP vector
    transcript: Optional[str]
    metadata: dict  # {"brightness": 0.8, "shot_type": "close_up"}
```

### Architecture Mapping

| Key Decision | Implementation Phase | Solution |
|--------------|----------------------|----------|
| **Multimodal Understanding** | **Phase 2** | Ingest pipeline extracts frames @ 1fps -> CLIP embeddings -> ChromaDB. Audio -> Whisper -> Text Search. |
| **Intent Translation** | **Phase 3** | LLM (GPT-4o) prompts acting as "Translators" converting natural language to specific MLT XML filter definitions and cut commands. |
| **Scene Detection** | **Phase 2** | Use `scenedetect` (PySceneDetect) for hard cuts + Embedding cosine similarity for "semantic" scene boundaries. |
| **EDL Generation** | **Phase 3** | The Agent generates an intermediate JSON EDL which is validated against the current timeline state before being converted to MLT API calls. |
| **Incremental Preview** | **Phase 4** | The Python agent identifies time ranges affected by the EDL. It instructs MLT to render *only* those ranges to a temporary buffer for instant playback. |
| **Feedback Incorporation** | **Phase 4** | LangGraph "Reflexion" node. If User rejects edit, the `feedback` edge loops back to the Planner with the user's critique added to the context window. |

---

## 3. Detailed Implementation Plan

### Phase 1: Foundation (The "Body")
**Goal**: Establish the editing engine, the Python control layer, and the communication bridge.
*   **File Structure**:
    *   `src/bridge/zmq_server.py`: The Python side of the IPC.
    *   `src/editor/shotcut_plugin/`: The C++/Qt code to send commands to Python.
    *   `src/engine/mlt_wrapper.py`: Python wrapper around MLT framework for timeline manipulation.
*   **Key Tasks**:
    *   Fork Shotcut and compile locally.
    *   Implement a simple "Hello World" plugin in Shotcut that sends a string to a running Python script via ZeroMQ.
    *   Create `MLTController` class in Python that can load a video file and perform a hard cut programmatically.
    *   Define the `TimelineState` schema and implement a function to serialize the current Shotcut timeline to JSON.

### Phase 2: Core Logic - Perception (The "Eyes & Ears")
**Goal**: Give the agent the ability to "see" the footage.
*   **File Structure**:
    *   `src/perception/indexer.py`: Main logic for processing video files.
    *   `src/perception/vision.py`: CLIP model wrapper.
    *   `src/perception/audio.py`: Whisper and CLAP wrappers.
    *   `src/db/chroma_client.py`: Vector database interface.
*   **Key Tasks**:
    *   Implement `ingest_video(path)`: Decodes video, extracts frames at 1Hz, extracts audio track.
    *   Implement `detect_scenes(video_path)`: Use PySceneDetect to find cut points.
    *   Integrate CLIP: Generate embeddings for each frame and store in ChromaDB.
    *   Integrate Whisper: Generate timestamped subtitles and store in ChromaDB.

### Phase 3: Agentic Editing (The "Brain")
**Goal**: Connect intent to action using LangGraph.
*   **File Structure**:
    *   `src/agent/graph.py`: The LangGraph state machine definition.
    *   `src/agent/prompts.py`: System prompts for "Director", "Editor", and "Critic" agents.
    *   `src/agent/tools.py`: Tools the agent can call (e.g., `apply_filter`, `cut_clip`).
*   **Key Tasks**:
    *   Define the LangGraph state: `messages`, `current_timeline`, `scratchpad`.
    *   Implement the `Planner` node: Takes user prompt + timeline context -> outputs high-level plan.
    *   Implement the `Executor` node: Takes plan -> outputs `MLTController` commands.
    *   Implement "Style Transfer" logic: Map keywords ("cinematic", "gloomy") to LUTs and Color Grading filters.

### Phase 4: Polish & Interaction (The "Soul")
**Goal**: UX, Preview, and Learning.
*   **File Structure**:
    *   `src/ui/chat_overlay.qml`: The frontend chat interface.
    *   `src/engine/preview_generator.py`: Logic for smart rendering.
    *   `src/memory/feedback_store.py`: Long-term user preference tracking.
*   **Key Tasks**:
    *   Build the Chat UI in Qt/QML that sits inside Shotcut.
    *   Implement "Why did you do that?" logic: Store the LLM's reasoning chain in a sidecar JSON linked to the Edit ID.
    *   Implement Incremental Preview: Render only the modified segments (±2 seconds handle) for quick review.

---

## 4. Development Priorities

| Priority | Task | Description |
|----------|------|-------------|
| **P0** | **MLT Python Control** | Prove we can programmatically cut video and apply filters via Python. Without this, nothing works. |
| **P0** | **Multimodal Indexing** | Get video frames and audio into a queryable vector format (ChromaDB). |
| **P1** | **LangGraph Agent** | Build the basic "Prompt -> Action" loop. |
| **P1** | **IPC Bridge** | Connect the customized Editor UI to the Python backend. |
| **P2** | **Incremental Preview** | Optimization for speed (can re-render full video for MVP). |
| **P2** | **Reasoning UI** | Visualizing the "Why" in the interface. |
| **P3** | **User Preference Learning** | Long-term memory of user style. |
